# Regularisation in Neural Networks: L2, Dropout, and Early Stopping

This repository contains the code, figures, and tutorial for a machine learning coursework project exploring the effect of regularisation techniques on multilayer perceptrons (MLPs). The tutorial compares L2 regularisation, dropout, and early stopping using a subset of the Fashion-MNIST dataset.

## Contents

- `notebook.ipynb` – The full Jupyter notebook used to run the experiments and generate all plots.
- `tutorial.pdf` – The final written tutorial (under 2000 words).
- `figures/` – All figures produced by the notebook and included in the PDF.
- `requirements.txt` – Python package requirements.
- `LICENSE` – Open-source license for this project.
- `README.md` – Repository documentation.

## How to Run the Notebook

1. Install Python 3.9+  
2. Install dependencies:
